import streamlit as st
import requests
from bs4 import BeautifulSoup
import time
import re


st.title("ğŸˆ NAVER Blog Scraping")

st.write("ë„¤ì´ë²„ ë¸”ë¡œê·¸ì˜ ë³¸ë¬¸ ë‚´ìš©ì„ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.")



import requests
from bs4 import BeautifulSoup
import re
import json

def convert_to_mobile_url(pc_url: str) -> str:
    """
    PC ë²„ì „ ë„¤ì´ë²„ ë¸”ë¡œê·¸ URLì„ ëª¨ë°”ì¼ ë²„ì „ URLë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ì˜ˆ: https://blog.naver.com/ì•„ì´ë””/í¬ìŠ¤íŠ¸ë²ˆí˜¸ -> https://m.blog.naver.com/ì•„ì´ë””/í¬ìŠ¤íŠ¸ë²ˆí˜¸
    """
    if "blog.naver.com" not in pc_url:
        raise ValueError("ìœ íš¨í•œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ URLì´ ì•„ë‹™ë‹ˆë‹¤.")
    # URLì´ ì´ë¯¸ ëª¨ë°”ì¼ ë²„ì „ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if "m.blog.naver.com" in pc_url:
        return pc_url

    # ê°„ë‹¨í•˜ê²Œ 'blog.naver.com'ì„ 'm.blog.naver.com'ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
    mobile_url = pc_url.replace("blog.naver.com", "m.blog.naver.com")
    return mobile_url

def scrape_naver_blog(pc_url: str) -> str:
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ PC ë²„ì „ URLì„ ë°›ì•„, ëª¨ë°”ì¼ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ HTMLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    mobile_url = convert_to_mobile_url(pc_url)

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/98.0.4758.102 Safari/537.36"
        )
    }
    response = requests.get(mobile_url, headers=headers)
    if not response.ok:
        raise ConnectionError(f"ëª¨ë°”ì¼ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")

    html_text = response.text
    soup = BeautifulSoup(html_text, "html.parser")

    # ì˜ˆì œ 1: ì¼ë°˜ì ì¸ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ divë¥¼ í™œìš©í•˜ëŠ” ê²½ìš°
    content_div = soup.find("div", {"class": "se-main-container"})
    if content_div:
        # return str(content_div)
        return content_div.get_text(separator='\n', strip=True)

    # ì˜ˆì œ 2: JSON ë°ì´í„°ê°€ í¬í•¨ëœ <script> íƒœê·¸ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ (ì˜ˆìƒ ë³€ìˆ˜ëª…ì´ __APOLLO_STATE__ ë“±)
    # ì•„ë˜ ì •ê·œì‹ì€ ì˜ˆì‹œì´ë©°, ì‹¤ì œ ë³€ìˆ˜ëª…ê³¼ êµ¬ì¡°ëŠ” HTML ì†ŒìŠ¤ í™•ì¸ í›„ ìˆ˜ì • í•„ìš”í•©ë‹ˆë‹¤.
    pattern = re.compile(r'window\.__APOLLO_STATE__\s*=\s*(\{.*?\});', re.DOTALL)
    match = pattern.search(html_text)
    if match:
        json_str = match.group(1).strip()
        try:
            data = json.loads(json_str)
            # ë°ì´í„° êµ¬ì¡°ì— ë”°ë¼ ìˆ˜ì • í•„ìš”: ì˜ˆì‹œë¡œ postContent í˜¹ì€ contentë¥¼ ì°¾ìŒ
            if "post" in data and "content" in data["post"]:
                return data["post"]["content"]
        except json.JSONDecodeError:
            pass  # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•„ë˜ ë°©ë²• ì‚¬ìš©

    # ì˜ˆì œ 3: iframe êµ¬ì¡°ì¸ ê²½ìš°, iframeì˜ srcë¥¼ ì¶”ì¶œí•˜ì—¬ ì¶”ê°€ ìš”ì²­
    iframe = soup.find("iframe")
    if iframe and iframe.has_attr("src"):
        iframe_src = iframe["src"]
        iframe_response = requests.get(iframe_src, headers=headers)
        if iframe_response.ok:
            iframe_soup = BeautifulSoup(iframe_response.text, "html.parser")
            # iframe ë‚´ì— ë³¸ë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸ (ì˜ˆ: div id="postViewArea")
            iframe_content = iframe_soup.find("div", {"id": "postViewArea"})
            if iframe_content:
                return str(iframe_content)

    raise ValueError("ë³¸ë¬¸ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. HTML êµ¬ì¡°ë¥¼ ì¬í™•ì¸í•´ ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    url = st.text_input("ë„¤ì´ë²„ ë¸”ë¡œê·¸ URLì„ ì…ë ¥í•˜ì„¸ìš”:", "https://blog.naver.com/ranto28/223839799372")

    try:
        content_html = scrape_naver_blog(url)
        st.write("=== ë³¸ë¬¸ HTML ===")
        st.write(content_html)
    except Exception as e:
        st.write(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
