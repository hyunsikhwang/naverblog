import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import json
import base64
import os
from openai import OpenAI
import time


api_key = st.secrets["api_key"]

# Minimal + Modern CSS ìŠ¤íƒ€ì¼ ì¶”ê°€
st.markdown("""
<style>
    /* Minimal + Modern Design */
    .main {
        background-color: #f8f9fa;
        font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
    }

    /* ì œëª© ìŠ¤íƒ€ì¼ */
    h1 {
        color: #2c3e50;
        font-weight: 600;
        margin-bottom: 1rem;
    }

    /* ì„œë¸Œí—¤ë” ìŠ¤íƒ€ì¼ */
    .stSubheader {
        color: #34495e;
        font-weight: 500;
        margin-top: 1.5rem;
        margin-bottom: 0.5rem;
        border-left: 4px solid #3498db;
        padding-left: 10px;
    }

    /* ì„ íƒ ë°•ìŠ¤ ìŠ¤íƒ€ì¼ */
    .stSelectbox > div > div {
        background-color: white;
        border-radius: 8px;
        border: 1px solid #e1e8ed;
    }

    /* í…ìŠ¤íŠ¸ ì˜ì—­ ìŠ¤íƒ€ì¼ */
    .stTextArea > div > div {
        background-color: white;
        border-radius: 8px;
        border: 1px solid #e1e8ed;
    }

    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .stButton > button {
        background-color: #3498db;
        color: white;
        border-radius: 8px;
        border: none;
        padding: 0.5rem 1.5rem;
        font-weight: 500;
        transition: all 0.3s ease;
    }

    .stButton > button:hover {
        background-color: #2980b9;
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }

    /* ì½”ë“œ ë¸”ë¡ ìŠ¤íƒ€ì¼ */
    .stCodeBlock {
        background-color: #f8f9fa;
        border-radius: 8px;
        border: 1px solid #e1e8ed;
    }

    /* ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .stDownloadButton > button {
        background-color: #2ecc71;
        color: white;
        border-radius: 8px;
        border: none;
        padding: 0.5rem 1.5rem;
        font-weight: 500;
        transition: all 0.3s ease;
    }

    .stDownloadButton > button:hover {
        background-color: #27ae60;
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }

    /* ìŠ¤í”¼ë„ˆ ìŠ¤íƒ€ì¼ */
    .stSpinner > div {
        border-top-color: #3498db;
    }

    /* ì¼ë°˜ í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ */
    .stMarkdown p {
        color: #7f8c8d;
        line-height: 1.6;
    }

    /* ë§í¬ ìŠ¤íƒ€ì¼ */
    a {
        color: #3498db;
        text-decoration: none;
    }

    a:hover {
        text-decoration: underline;
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸˆ NAVER Blog Scraping")

st.write("ë„¤ì´ë²„ ë¸”ë¡œê·¸ì˜ ë³¸ë¬¸ ë‚´ìš©ì„ ìŠ¤í¬ë˜í•‘í•˜ê³ , OpenRouterë¥¼ í†µí•´ í•œì¤„ ì½”ë©˜íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")


def fetch_post_list(category_no=0, item_count=24, page=1, user_id="gomting"):
    """
    ë„¤ì´ë²„ ëª¨ë°”ì¼ ë¸”ë¡œê·¸ APIì—ì„œ í¬ìŠ¤íŠ¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        category_no (int): ì¹´í…Œê³ ë¦¬ ë²ˆí˜¸ (ê¸°ë³¸ê°’: 0)
        item_count (int): í•œ ë²ˆì— ê°€ì ¸ì˜¬ ì•„ì´í…œ ìˆ˜ (ê¸°ë³¸ê°’: 24)
        page (int): í˜ì´ì§€ ë²ˆí˜¸ (ê¸°ë³¸ê°’: 1)
        user_id (str): ì‚¬ìš©ì ID (ì˜ˆ: "gomting")
    
    Returns:
        dict or None: JSON íŒŒì‹± ê²°ê³¼, ì‹¤íŒ¨ ì‹œ None
    """
    url = "https://m.blog.naver.com/api/blogs/ranto28/post-list"
    params = {
        "categoryNo": category_no,
        "itemCount": item_count,
        "page": page,
        "userId": user_id
    }
    # ì£¼ì–´ì§„ ëª¨ë“  í—¤ë”ë¥¼ ê·¸ëŒ€ë¡œ ë°˜ì˜
    headers = {
        "Host": "m.blog.naver.com",
        "Accept": "application/json, text/plain, */*",
        "Accept-Encoding": "gzip, deflate, br, zstd",
        "Accept-Language": "ko,en-US;q=0.9,en;q=0.8",
        # "Cookie": (
        #     "NNB=7M2TKPQSXKWGC; stat_yn=1; BMR=s=1678195925790&r=https%3A%2F%2Fm.post.naver.com%2Fviewer%2FpostView.naver%3FmemberNo%3D37966086%26volumeNo%3D35583566"
        #     "&r2=https%3A%2F%2Fwww.naver.com%2Fmy.html; ba.uuid=0; m_loc=11130e4b1b9a413a653df5ca74a909e251462cda8340fec489dba5f19fc140ca;"
        #     'NV_WETR_LOCATION_RGN_M="MDIzNjAyNTY="; tooltipDisplayed=true; NFS=2; BA_DEVICE=61a4ec76-de32-4960-a16d-c11fc9aaef73;'
        #     'NV_WETR_LAST_ACCESS_RGN_M="MDIzNjAyNTY="; nstore_session=pvYgGdc32RjrCi05sB8wWxA1; NID_AUT=bns3r5WpId46252AZmVwF64qnag9m4gF0QT9UQSdLmAAeZop4s0iV31BzJ6lB86D;'
        #     "NID_JKL=h9Qg0W2Qc8jPtOMZvp/zrCMkSOzmP0SYyC/yZ+D3Q74=; ASID=738c015f000001947848ed1000000055; BNB_FINANCE_HOME_TOOLTIP_STOCK=true;"
        #     "BNB_FINANCE_HOME_TOOLTIP_ESTATE=true; _ga=GA1.1.1795909208.1738987026; naverfinancial_CID=b3315d5dea424c45e3b8fb63a8f0f03a; _gcl_au=1.1.742333026.1738987026;"
        #     "_tt_enable_cookie=1; _ttp=zLLgXYt97Q_3LsR_-nOWlsvuJZw.tt.1; _ga_Q7G1QTKPGB=GS1.1.1738987025.1.1.1738987043.0.0.0; NAC=r4UQBkQAcMI9;"
        #     "_ga_K2ECMCJBFQ=GS1.1.1745140624.1.0.1745140629.0.0.0; _ga_SQ24F7Q7YW=GS1.1.1745140624.1.0.1745140629.0.0.0; JSESSIONID=E299B5F7A1237F4BBB6BBA65B118899D.jvm1;"
        #     "SRT30=1746515923; page_uid=jtEI+wqo1fsssS855/wssssssAh-373171; NID_SES=AAABw+OSE8kAfR+cfS6+AGOLvkusjXoMrXSguKUGlZuS4wqvCr71CksIxzQ1Ec6aHeeyi3MwCCnq98jHXuAhug8HYzfsnWljppjjR1wnxfjuqCaigbwJOGTq8/Q05fR89QlGovxXVx1Ye/XUqy5lDtyIdRYsxfIeWBZjzGAc/xllozHTXA7flWSQ10ca0+C3oVEpaFPVWXLvDQlkHjzDGFpJBoJMbxml8/Aqgncw7OjyuJViF51a/D+ih28z6JUJkBARcxarnNURq1v4UD7LWW+jFtIamMIVbiFO3HsU64BvZyp/sNnt/8s017umcADw1fv5g25bWiHGnSrsbZsRdNNeaHUcIymCIbDCnfO+eBmUsR7NlvJKKJFK6a6XsN/5KKkNegQbQoy3GMaY2AIibDCCSquwmBnzSam5jE50p28EGDMoHNWZLxvoeEUQe1/E1fgksQZhNI99FYoa5f+gQjAYCOsB/ZOXo+tFxp1pJYzKv/aDDNBaBg6WSukzdzDxljE2tRZK2BfsWhbUpTbxr+aclIKSNt+M8XFcQvxYF51fvfzg4xlhFdpuZLNp3klyOVnTRTKTuIjzIAw94tZbl+CRzalDDtIBSJZIzWHm6DL/4EOe; nstore_pagesession=jtE/3sqrZpJVcwsMDZd-358989; SRT5=1746519782; BUC=E_CivUq4tO2ACvTvfwivm5O-ay0RIneThGnZi6r8Uwo="
        # ),
        "Referer": "https://m.blog.naver.com/ranto28?categoryNo=0&tab=1",
        "Sec-CH-UA": '"Google Chrome";v="135", "Not-A.Brand";v="8", "Chromium";v="135"',
        "Sec-CH-UA-Mobile": "?0",
        "Sec-CH-UA-Platform": '"macOS"',
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "same-origin",
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36",
        "Priority": "u=1, i"
    }

    try:
        resp = requests.get(url, params=params, headers=headers, timeout=10)
        resp.raise_for_status()
        return resp.json()
    except requests.RequestException as e:
        print(f"ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    except ValueError:
        print("ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return None

def print_blog_summary(response):
    links = {}
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ JSON ì‘ë‹µì—ì„œ ì£¼ìš” í•­ëª©ë§Œ ê°„ê²°í•˜ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    # ìš”ì²­ ì„±ê³µ ì—¬ë¶€ í™•ì¸
    if not response.get('isSuccess', False):
        print("ìš”ì²­ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return

    result = response.get('result', {})
    items = result.get('items', [])

    if not items:
        print("í‘œì‹œí•  ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    for item in items:
        blog_id = item.get('domainIdOrBlogId')
        log_no = item.get('logNo')
        title = item.get('titleWithInspectMessage', '<ì œëª© ì—†ìŒ>')
        comments = item.get('commentCnt', 0)
        sympathies = item.get('sympathyCnt', 0)
        # ë³¸ë¬¸ì€ ì²« ë¬¸ì¥ë§Œ ì¶”ì¶œí•´ ê°„ëµí•˜ê²Œ ë³´ì—¬ì¤ë‹ˆë‹¤.
        brief = item.get('briefContents', '').split('ã€‚')[0]
        link = f"https://m.blog.naver.com/{blog_id}/{log_no}"

        # links.append(f"{link}")
        links[f"{title}"] = f"{link}"
        # print(f"ì œëª©       : {title}")
        # print(f"ë§í¬       : {link}")
        # print(f"ëŒ“ê¸€/ê³µê°  : {comments}ê°œ  /  {sympathies}ê°œ")
        # print(f"ìš”ì•½       : {brief}â€¦")
        # print("-" * 60)
    
    return links

def convert_to_mobile_url(pc_url: str) -> str:
    """
    PC ë²„ì „ ë„¤ì´ë²„ ë¸”ë¡œê·¸ URLì„ ëª¨ë°”ì¼ ë²„ì „ URLë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ì˜ˆ: https://blog.naver.com/ì•„ì´ë””/í¬ìŠ¤íŠ¸ë²ˆí˜¸ -> https://m.blog.naver.com/ì•„ì´ë””/í¬ìŠ¤íŠ¸ë²ˆí˜¸
    """
    if "blog.naver.com" not in pc_url:
        raise ValueError("ìœ íš¨í•œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ URLì´ ì•„ë‹™ë‹ˆë‹¤.")
    # URLì´ ì´ë¯¸ ëª¨ë°”ì¼ ë²„ì „ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if "m.blog.naver.com" in pc_url:
        return pc_url

    # ê°„ë‹¨í•˜ê²Œ 'blog.naver.com'ì„ 'm.blog.naver.com'ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
    mobile_url = pc_url.replace("blog.naver.com", "m.blog.naver.com")
    return mobile_url

def scrape_naver_blog(pc_url: str) -> str:
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ PC ë²„ì „ URLì„ ë°›ì•„, ëª¨ë°”ì¼ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ HTMLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    mobile_url = convert_to_mobile_url(pc_url)

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/98.0.4758.102 Safari/537.36"
        )
    }
    response = requests.get(mobile_url, headers=headers)
    if not response.ok:
        raise ConnectionError(f"ëª¨ë°”ì¼ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")

    html_text = response.text
    soup = BeautifulSoup(html_text, "html.parser")

    # ì˜ˆì œ 1: ì¼ë°˜ì ì¸ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ divë¥¼ í™œìš©í•˜ëŠ” ê²½ìš°
    content_div = soup.find("div", {"class": "se-main-container"})
    if content_div:
        # return str(content_div)
        return content_div.get_text(separator='\n', strip=True)

    # ì˜ˆì œ 2: JSON ë°ì´í„°ê°€ í¬í•¨ëœ <script> íƒœê·¸ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ (ì˜ˆìƒ ë³€ìˆ˜ëª…ì´ __APOLLO_STATE__ ë“±)
    # ì•„ë˜ ì •ê·œì‹ì€ ì˜ˆì‹œì´ë©°, ì‹¤ì œ ë³€ìˆ˜ëª…ê³¼ êµ¬ì¡°ëŠ” HTML ì†ŒìŠ¤ í™•ì¸ í›„ ìˆ˜ì • í•„ìš”í•©ë‹ˆë‹¤.
    pattern = re.compile(r'window\.__APOLLO_STATE__\s*=\s*(\{.*?\});', re.DOTALL)
    match = pattern.search(html_text)
    if match:
        json_str = match.group(1).strip()
        try:
            data = json.loads(json_str)
            # ë°ì´í„° êµ¬ì¡°ì— ë”°ë¼ ìˆ˜ì • í•„ìš”: ì˜ˆì‹œë¡œ postContent í˜¹ì€ contentë¥¼ ì°¾ìŒ
            if "post" in data and "content" in data["post"]:
                return data["post"]["content"]
        except json.JSONDecodeError:
            pass  # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•„ë˜ ë°©ë²• ì‚¬ìš©

    # ì˜ˆì œ 3: iframe êµ¬ì¡°ì¸ ê²½ìš°, iframeì˜ srcë¥¼ ì¶”ì¶œí•˜ì—¬ ì¶”ê°€ ìš”ì²­
    iframe = soup.find("iframe")
    if iframe and iframe.has_attr("src"):
        iframe_src = iframe["src"]
        iframe_response = requests.get(iframe_src, headers=headers)
        if iframe_response.ok:
            iframe_soup = BeautifulSoup(iframe_response.text, "html.parser")
            # iframe ë‚´ì— ë³¸ë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸ (ì˜ˆ: div id="postViewArea")
            iframe_content = iframe_soup.find("div", {"id": "postViewArea"})
            if iframe_content:
                return str(iframe_content)

    raise ValueError("ë³¸ë¬¸ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. HTML êµ¬ì¡°ë¥¼ ì¬í™•ì¸í•´ ì£¼ì„¸ìš”.")

def insert_line_breaks(text):
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì•„ë˜ ì„¸ ê²½ìš°ì— ì¤„ë°”ê¿ˆì„ ì‚½ì…í•©ë‹ˆë‹¤:
    1) ë¬¸ì¥ì´ ë§ˆì¹¨í‘œë¡œ ëë‚œ ì§í›„
    2) ì¼ë ¨ë²ˆí˜¸(ì˜ˆ: '1.')ê°€ ë‚˜ì˜¤ê¸° ë°”ë¡œ ì•
    3) 'í•œì¤„ì½”ë©˜íŠ¸' ë˜ëŠ” 'í•œì¤„ ì½”ë©˜íŠ¸'ê°€ ë‚˜ì˜¤ê¸° ë°”ë¡œ ì§ì „
    """
    # 1) ë¬¸ì¥ì´ ë§ˆì¹¨í‘œë¡œ ëë‚œ ì§í›„: '...' ë’¤ì— \n ì¶”ê°€
    #    (?<!\n) ìœ¼ë¡œ ì´ë¯¸ ì¤„ë°”ê¿ˆì´ ì—†ëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
    text = re.sub(r'(?<!\n)\.(?=\s|$)', r'.\n', text)

    # 2) ì¼ë ¨ë²ˆí˜¸ ì•: 'ìˆ«ì+.' ì•ì— \n ì¶”ê°€
    text = re.sub(r'(?<!\n)(?=(\d+\.))', r'\n', text)

    # 3) 'í•œì¤„ì½”ë©˜íŠ¸' ë˜ëŠ” 'í•œì¤„ ì½”ë©˜íŠ¸' ë°”ë¡œ ì§ì „: ì•ì— \n ì¶”ê°€
    text = re.sub(r'(?<!\n)(?=(í•œì¤„\s*ì½”ë©˜íŠ¸))', r'\n', text)

    return text

def generate(api_key, content_html):

    text = """ë‹¤ìŒ ì›ë¬¸ì—ì„œ í•œì¤„ ì½”ë©˜íŠ¸ë§Œ ì •í™•í•˜ê²Œ ì¶”ì¶œí•´ì„œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
í•œì¤„ ì½”ë©˜íŠ¸: {í•œì¤„ ì½”ë©˜íŠ¸}
ì›ë¬¸: """+content_html

    client = OpenAI(
        api_key=api_key,
        base_url="https://openrouter.ai/api/v1",
    )

    model = "xiaomi/mimo-v2-flash:free"

    stream = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "user", "content": text},
        ],
        stream=True,
    )

    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            yield chunk.choices[0].delta.content
        time.sleep(0.01)

def get_full_response(api_key, content_html):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì™„ì „í•œ í…ìŠ¤íŠ¸ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    full_response = ""
    for chunk in generate(api_key, content_html):
        full_response += chunk
    return full_response

def extract_comment(full_response):
    """ì‘ë‹µì—ì„œ í•œì¤„ ì½”ë©˜íŠ¸ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    comment_match = re.search(r'í•œì¤„ ì½”ë©˜íŠ¸:\s*(.*?)(?=\nì›ë¬¸:|$)', full_response, re.DOTALL)
    comment = comment_match.group(1).strip() if comment_match else "í•œì¤„ ì½”ë©˜íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    return comment

def remove_blank_lines(text: str) -> str:
    # 1) Zeroâ€‘width space, BOM ê°™ì€ ë³´ì´ì§€ ì•ŠëŠ” ë¬¸ì ì œê±°
    for ch in ('\u200B', '\uFEFF'):
        text = text.replace(ch, '')

    # 2) ì¤„ êµ¬ë¶„ì„ ëª¨ë‘ '\n' ìœ¼ë¡œ í†µì¼
    text = text.replace('\r\n', '\n').replace('\r', '\n')

    # 3) ì™„ì „íˆ ê³µë°±ë§Œ ìˆëŠ” ì¤„(íƒ­, ìŠ¤í˜ì´ìŠ¤, NBSP ë“± í¬í•¨) ì œê±°
    #    (?m) ë©€í‹°ë¼ì¸ ëª¨ë“œ, ^â€¦$ ì— \s ë¥¼ ì¨ì„œ ìœ ë‹ˆì½”ë“œ ê³µë°± ì „ë¶€ ë§¤ì¹­
    cleaned = re.sub(r'(?m)^[\s\u00A0]*\n', '', text)

    # 4) ì•ë’¤ì— ë‚¨ì€ ë¹ˆ ì¤„/ê³µë°± ì˜ë¼ë‚´ê¸°
    return cleaned.strip()

if __name__ == "__main__":

    response = fetch_post_list()
    if response:
        links = print_blog_summary(response)
        if links:  # linksê°€ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
            titles = list(links.keys())
        else:
            st.write("ê²Œì‹œê¸€ ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            st.stop()
    else:
        st.write("ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    url = st.selectbox("ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš”:", titles)
    st.write(f"ì„ íƒí•œ URL: {links[url]}")

    try:
        content_html = scrape_naver_blog(links[url])

        # ë¶ˆí•„ìš”í•œ ë¹ˆ ì¤„ ì œê±°
        content_html = remove_blank_lines(content_html)

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì™„ì „í•œ í…ìŠ¤íŠ¸ë¡œ ìˆ˜ì§‘
        with st.spinner("OpenRouter ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘..."):
            full_response = get_full_response(api_key, content_html)

        # í•œì¤„ ì½”ë©˜íŠ¸ ì¶”ì¶œ
        comment = extract_comment(full_response)

        # í•œì¤„ ì½”ë©˜íŠ¸ ì¶œë ¥
        st.subheader("ğŸ“ í•œì¤„ ì½”ë©˜íŠ¸")
        st.write(comment)

        # ì›ë¬¸ ì¶œë ¥ (ì •ë¦¬ëœ content_html ì‚¬ìš©)
        st.subheader("ğŸ“„ ì›ë¬¸")

        # ëŒ€ì•ˆ: ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ í‘œì‹œí•˜ì—¬ ë³µì‚¬ ê°€ëŠ¥í•˜ê²Œ í•¨
        st.code(content_html, language="text")

        st.text_area("ì›ë¬¸ ë‚´ìš©", content_html, height=300, key="original_text")

        # í´ë¦½ë³´ë“œ ë³µì‚¬ ê¸°ëŠ¥ (Streamlit ì œí•œìœ¼ë¡œ ì¸í•´ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì œê³µ)
        st.download_button(
            label="ì›ë¬¸ ë³µì‚¬ (í…ìŠ¤íŠ¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ)",
            data=content_html,
            file_name="original_text.txt",
            mime="text/plain"
        )

    except Exception as e:
        st.write(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
